#ifndef NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H
#define NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H

enum InferenceBackend {
    LIBTORCH,
    ONNX,
    TFLITE
};

#define MODEL_TFLITE "model_0/model_0-streaming.tflite"
#define MODEL_LIBTORCH "model_0/model_0-streaming.pt"
#define MODELS_PATH_ONNX MODELS_PATH_TENSORFLOW
#define MODEL_ONNX "model_0/model_0-tflite-streaming.onnx"


#define BATCH_SIZE 128
#define MODEL_INPUT_SIZE 1
#define MODEL_INPUT_SIZE_BACKEND 150 // Same as MODEL_INPUT_SIZE, but for streamable models
#define MODEL_INPUT_SHAPE_ONNX {BATCH_SIZE, MODEL_INPUT_SIZE_BACKEND, 1}
#define MODEL_INPUT_SHAPE_TFLITE {BATCH_SIZE, MODEL_INPUT_SIZE_BACKEND, 1}
#define MODEL_INPUT_SHAPE_LIBTORCH {BATCH_SIZE, 1, MODEL_INPUT_SIZE_BACKEND}


#define MODEL_OUTPUT_SIZE_BACKEND 1
#define MODEL_OUTPUT_SHAPE {BATCH_SIZE, MODEL_OUTPUT_SIZE_BACKEND}


#if WIN32
#define MAX_INFERENCE_TIME 8192
#else
#define MAX_INFERENCE_TIME 256
#endif

#define MODEL_LATENCY 0

namespace NNInferenceTemplate {
    using InputArray = std::array<float, BATCH_SIZE * MODEL_INPUT_SIZE_BACKEND>;
    using OutputArray = std::array<float, BATCH_SIZE * MODEL_OUTPUT_SIZE_BACKEND>;
}

#endif //NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H